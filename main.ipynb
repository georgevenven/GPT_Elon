{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a decoder only transformer for generating Elon Musk tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Elon Tweets (test dataset), cleaning up, and tokenizing with dict representing the vocab, also np arr will be saved with all the tweet combination lengths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12562, 426)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('TweetsElonMusk.csv')\n",
    "df = df['tweet']\n",
    "\n",
    "# number of df rows\n",
    "n = len(df)\n",
    "\n",
    "# take all rows and concat them into one string\n",
    "text = ''\n",
    "for i in range(n):\n",
    "    text += df[i]\n",
    "\n",
    "# find unique chars in text\n",
    "set_of_unique_chars = set(text)\n",
    "\n",
    "# create a dictionary of unique chars, mapping each char to an int\n",
    "char_to_int = {}\n",
    "\n",
    "for i, char in enumerate(set_of_unique_chars):\n",
    "    char_to_int[char] = i\n",
    "\n",
    "# add 395 as <sos> token, 396 as <eos> token, 397 as <pad> token\n",
    "char_to_int['<sos>'] = 395\n",
    "char_to_int['<eos>'] = 396\n",
    "char_to_int['<pad>'] = 397\n",
    "\n",
    "# find row with longest string\n",
    "max_len = 0\n",
    "for i in range(n):\n",
    "    if len(df[i]) > max_len:\n",
    "        max_len = len(df[i])\n",
    "\n",
    "elon_data = np.zeros((n, max_len + 2))\n",
    "\n",
    "print(elon_data.shape)\n",
    "\n",
    "# 395 is the sos token and 396 is the eos token, 397 is the padding token\n",
    "for i in range(n):\n",
    "    elon_data[i][0] = 395\n",
    "    for j in range(len(df[i])):\n",
    "        elon_data[i][j + 1] = char_to_int[df[i][j]]\n",
    "    elon_data[i][len(df[i]) + 1] = 396\n",
    "    elon_data[i][len(df[i]) + 2:] = 397\n",
    "\n",
    "# save to np file\n",
    "np.save('elon_data.npy', elon_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397\n"
     ]
    }
   ],
   "source": [
    "print(char_to_int['<pad>'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions for converting tokens into chars and chars into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_chars(tokens):\n",
    "    chars = []\n",
    "    for token in tokens:\n",
    "        for char in char_to_int:\n",
    "            if char_to_int[char] == token:\n",
    "                chars.append(char)\n",
    "    return chars\n",
    "\n",
    "def chars_to_tokens(chars):\n",
    "    tokens = []\n",
    "    for char in chars:\n",
    "        tokens.append(char_to_int[char])\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "architecture parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 3\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "embedding_dim = 64\n",
    "dict_size = len(char_to_int)\n",
    "attention_dim = 16\n",
    "feed_forward_dim = embedding_dim\n",
    "num_heads = 8 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder():\n",
    "    def __init__(self,layers, epochs, batch_size, embedding_dim, dict_size, feed_forward_dim, num_heads, attention_dim):\n",
    "        self.layers = layers\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dict_size = dict_size\n",
    "        self.feed_forward_dim = feed_forward_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=dict_size+1, embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.decoder_layer_1 = DecoderLayer(self.embedding_dim, self.attention_dim, self.feed_forward_dim, self.num_heads)\n",
    "        self.decoder_layer_2 = DecoderLayer(self.embedding_dim, self.attention_dim, self.feed_forward_dim, self.num_heads)\n",
    "        self.decoder_layer_3 = DecoderLayer(self.embedding_dim, self.attention_dim, self.feed_forward_dim, self.num_heads)\n",
    "\n",
    "        self.linear = nn.Linear(in_features=embedding_dim, out_features=dict_size+1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        positional_encodings = self.positional_encoding(len(x), self.embedding_dim)\n",
    "        x += positional_encodings\n",
    "        \n",
    "        x = self.decoder_layer_1.forward(x)\n",
    "\n",
    "        pass\n",
    "\n",
    "    ## This was generated by chat gpt-3, so hopefully it works\n",
    "    def positional_encoding(self, seq_len, embedding_dim):\n",
    "        positions = np.arange(seq_len)[:, np.newaxis]\n",
    "        angles = np.power(10000, -(2 * (np.arange(embedding_dim) // 2) / embedding_dim))\n",
    "        angles = angles[np.newaxis, :]\n",
    "\n",
    "        positional_encodings = positions * angles\n",
    "\n",
    "        # Apply sine to even indices in the array; 2i\n",
    "        positional_encodings[:, 0::2] = np.sin(positional_encodings[:, 0::2])\n",
    "\n",
    "        # Apply cosine to odd indices in the array; 2i+1\n",
    "        positional_encodings[:, 1::2] = np.cos(positional_encodings[:, 1::2])\n",
    "\n",
    "        positional_encodings = torch.from_numpy(positional_encodings).float()\n",
    "        return positional_encodings\n",
    "\n",
    "class DecoderLayer(nn.Module):    \n",
    "    def __init__(self, embedding_dim, attention_dim, feed_forward_dim, num_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(embedding_dim=embedding_dim, attention_dim=attention_dim, num_heads=num_heads)\n",
    "        self.feed_forward = FeedForward()\n",
    "        self.layer_norm = LayerNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.multi_head_attention.forward(x)\n",
    "        return x \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, attention_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # linear transform from embedding -> attention_dim \n",
    "        # later divide into attention heads \n",
    "        self.Q = nn.Linear(in_features=embedding_dim, out_features=attention_dim * num_heads)\n",
    "        self.K = nn.Linear(in_features=embedding_dim, out_features=attention_dim * num_heads)\n",
    "        self.V = nn.Linear(in_features=embedding_dim, out_features=attention_dim * num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q_vectors = self.Q(x)\n",
    "        K_vectors = self.K(x)\n",
    "        V_vectors = self.V(x)\n",
    "\n",
    "        # this results in the shape sequence length, num_heads, attention_dim\n",
    "        Q_vectors = torch.reshape(Q_vectors, (x.shape[0], self.num_heads, self.attention_dim))\n",
    "        K_vectors = torch.reshape(K_vectors, (x.shape[0], self.num_heads, self.attention_dim))\n",
    "        V_vectors = torch.reshape(V_vectors, (x.shape[0], self.num_heads, self.attention_dim))\n",
    "\n",
    "        # now reshape into atten num heads, sequence length, attention dim\n",
    "        Q_vectors = Q_vectors.transpose(0,1)\n",
    "        K_vectors = K_vectors.transpose(0,1)\n",
    "        V_vectors = V_vectors.transpose(0,1)\n",
    "\n",
    "        # dot product of Q and K but not first dimension\n",
    "        attention_scores = torch.matmul(Q_vectors, K_vectors.transpose(1,2))\n",
    "        attention_scores = attention_scores / np.sqrt(self.attention_dim)\n",
    "\n",
    "        # now the shape is attention head, sequence length, softmax values \n",
    "\n",
    "        # softmax over attention dim \n",
    "        attention_scores = nn.Softmax(dim=2)(attention_scores)\n",
    "\n",
    "        # Multiply the value vectors by the attention scores {possible point of error}\n",
    "        weighted_sum = torch.matmul(attention_scores, V_vectors)\n",
    "\n",
    "        # Concatenate the heads back together\n",
    "        concatenated = weighted_sum.transpose(0, 1).reshape(x.shape[0], -1, self.attention_dim * self.num_heads)\n",
    "\n",
    "        return concatenated\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForward, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forwards(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 426, 16])\n",
      "torch.Size([426, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(layers=layers, epochs=epochs, batch_size=batch_size, embedding_dim=embedding_dim, dict_size=dict_size, feed_forward_dim=feed_forward_dim, num_heads=num_heads, attention_dim=attention_dim)\n",
    "\n",
    "# numpy arr to tensor\n",
    "test_tensor = torch.from_numpy(elon_data[0])\n",
    "test_tensor = test_tensor.int()\n",
    "\n",
    "decoder.forward(test_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
