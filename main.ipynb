{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a decoder only transformer for generating Elon Musk tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Elon Tweets (test dataset), cleaning up, and tokenizing with dict representing the vocab, also np arr will be saved with all the tweet combination lengths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('TweetsElonMusk.csv')\n",
    "df = df['tweet']\n",
    "\n",
    "# number of df rows\n",
    "n = len(df)\n",
    "\n",
    "# take all rows and concat them into one string\n",
    "text = ''\n",
    "for i in range(n):\n",
    "    text += df[i]\n",
    "\n",
    "# find unique chars in text\n",
    "set_of_unique_chars = set(text)\n",
    "\n",
    "# create a dictionary of unique chars, mapping each char to an int\n",
    "char_to_int = {}\n",
    "\n",
    "for i, char in enumerate(set_of_unique_chars):\n",
    "    char_to_int[char] = i\n",
    "\n",
    "# add 395 as <sos> token, 396 as <eos> token, 397 as <pad> token\n",
    "char_to_int['<sos>'] = 395\n",
    "char_to_int['<eos>'] = 396\n",
    "char_to_int['<pad>'] = 397\n",
    "\n",
    "# find row with longest string\n",
    "max_len = 0\n",
    "for i in range(n):\n",
    "    if len(df[i]) > max_len:\n",
    "        max_len = len(df[i])\n",
    "\n",
    "elon_data = np.zeros((n, max_len + 2))\n",
    "\n",
    "print(elon_data.shape)\n",
    "\n",
    "# 395 is the sos token and 396 is the eos token, 397 is the padding token\n",
    "for i in range(n):\n",
    "    elon_data[i][0] = 395\n",
    "    for j in range(len(df[i])):\n",
    "        elon_data[i][j + 1] = char_to_int[df[i][j]]\n",
    "    elon_data[i][len(df[i]) + 1] = 396\n",
    "    elon_data[i][len(df[i]) + 2:] = 397\n",
    "\n",
    "# save to np file\n",
    "np.save('elon_data.npy', elon_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(char_to_int['<pad>'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions for converting tokens into chars and chars into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_chars(tokens):\n",
    "    chars = []\n",
    "    for token in tokens:\n",
    "        for char in char_to_int:\n",
    "            if char_to_int[char] == token:\n",
    "                chars.append(char)\n",
    "    return chars\n",
    "\n",
    "def chars_to_tokens(chars):\n",
    "    tokens = []\n",
    "    for char in chars:\n",
    "        tokens.append(char_to_int[char])\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "architecture parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 3\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "embedding_dim = 64\n",
    "dict_size = len(char_to_int)\n",
    "attention_dim = 16\n",
    "feed_forward_dim = embedding_dim\n",
    "num_heads = 8 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,layers, epochs, batch_size, embedding_dim, dict_size, feed_forward_dim, num_heads, attention_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dict_size = dict_size\n",
    "        self.feed_forward_dim = feed_forward_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=dict_size+1, embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.decoder_layer_1 = DecoderLayer(self.embedding_dim, self.attention_dim, self.feed_forward_dim, self.num_heads)\n",
    "        self.decoder_layer_2 = DecoderLayer(self.embedding_dim, self.attention_dim, self.feed_forward_dim, self.num_heads)\n",
    "        self.decoder_layer_3 = DecoderLayer(self.embedding_dim, self.attention_dim, self.feed_forward_dim, self.num_heads)\n",
    "\n",
    "        self.linear = nn.Linear(in_features=embedding_dim, out_features=dict_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        positional_encodings = self.positional_encoding(len(x), self.embedding_dim)\n",
    "\n",
    "        x += positional_encodings\n",
    "        \n",
    "        x = self.decoder_layer_1.forward(x)\n",
    "        x = self.decoder_layer_2.forward(x)\n",
    "        x = self.decoder_layer_3.forward(x)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "    ## This was generated by chat gpt-3, so hopefully it works\n",
    "    def positional_encoding(self, seq_len, embedding_dim):\n",
    "        positions = np.arange(seq_len)[:, np.newaxis]\n",
    "        angles = np.power(10000, -(2 * (np.arange(embedding_dim) // 2) / embedding_dim))\n",
    "        angles = angles[np.newaxis, :]\n",
    "\n",
    "        positional_encodings = positions * angles\n",
    "\n",
    "        # Apply sine to even indices in the array; 2i\n",
    "        positional_encodings[:, 0::2] = np.sin(positional_encodings[:, 0::2])\n",
    "\n",
    "        # Apply cosine to odd indices in the array; 2i+1\n",
    "        positional_encodings[:, 1::2] = np.cos(positional_encodings[:, 1::2])\n",
    "\n",
    "        positional_encodings = torch.from_numpy(positional_encodings).float()\n",
    "        return positional_encodings\n",
    "\n",
    "class DecoderLayer(nn.Module):    \n",
    "    def __init__(self, embedding_dim, attention_dim, feed_forward_dim, num_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(embedding_dim=embedding_dim, attention_dim=attention_dim, num_heads=num_heads)\n",
    "        self.layer_norm_1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embedding_dim)\n",
    "        self.feed_forward_1 = nn.Linear(in_features=embedding_dim, out_features=feed_forward_dim)\n",
    "        self.feed_forward_2 = nn.Linear(in_features=feed_forward_dim, out_features=embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.multi_head_attention.forward(x)\n",
    "        x = self.layer_norm_1(x + attention)\n",
    "        feed_forward = self.feed_forward_2(F.relu(self.feed_forward_1(x)))\n",
    "        x = self.layer_norm_2(x + feed_forward)\n",
    "        return x \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, attention_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # linear transform from embedding -> attention_dim \n",
    "        # later divide into attention heads \n",
    "        self.Q = nn.Linear(in_features=embedding_dim, out_features=attention_dim * num_heads)\n",
    "        self.K = nn.Linear(in_features=embedding_dim, out_features=attention_dim * num_heads)\n",
    "        self.V = nn.Linear(in_features=embedding_dim, out_features=attention_dim * num_heads)\n",
    "\n",
    "        # reduce the attention heads back to the embedding dim size \n",
    "        self.Reduction = nn.Linear(in_features=attention_dim * num_heads, out_features=embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q_vectors = self.Q(x)\n",
    "        K_vectors = self.K(x)\n",
    "        V_vectors = self.V(x)\n",
    "\n",
    "        # this results in the shape sequence length, num_heads, attention_dim\n",
    "        Q_vectors = torch.reshape(Q_vectors, (x.shape[0], self.num_heads, self.attention_dim))\n",
    "        K_vectors = torch.reshape(K_vectors, (x.shape[0], self.num_heads, self.attention_dim))\n",
    "        V_vectors = torch.reshape(V_vectors, (x.shape[0], self.num_heads, self.attention_dim))\n",
    "\n",
    "        # now reshape into atten num heads, sequence length, attention dim\n",
    "        Q_vectors = Q_vectors.transpose(0,1)\n",
    "        K_vectors = K_vectors.transpose(0,1)\n",
    "        V_vectors = V_vectors.transpose(0,1)\n",
    "\n",
    "        # dot product of Q and K but not first dimension\n",
    "        attention_scores = torch.matmul(Q_vectors, K_vectors.transpose(1,2))\n",
    "        attention_scores = attention_scores / np.sqrt(self.attention_dim)\n",
    "\n",
    "        # now the shape is attention head, sequence length, softmax values \n",
    "\n",
    "        # softmax over attention dim \n",
    "        attention_scores = nn.Softmax(dim=2)(attention_scores)\n",
    "\n",
    "        # Multiply the value vectors by the attention scores {possible point of error}\n",
    "        weighted_sum = torch.matmul(attention_scores, V_vectors)\n",
    "\n",
    "        # Concatenate the heads back together\n",
    "        concatenated = weighted_sum.transpose(0, 1).reshape(x.shape[0], -1, self.attention_dim * self.num_heads)\n",
    "\n",
    "        # Linearly transform the concatenated vectors back to the embedding size\n",
    "        output = self.Reduction(concatenated)\n",
    "\n",
    "        # remove the middle 1 dimension\n",
    "        output = torch.squeeze(output, dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define a custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Convert the data to integer tensors\n",
    "        tensor_data = torch.tensor(self.data[index], dtype=torch.int32)\n",
    "\n",
    "        return tensor_data\n",
    "\n",
    "# load elon data npy\n",
    "elon_data = np.load('elon_data.npy')\n",
    "\n",
    "# Convert your data to integer tensors\n",
    "int_data = [[int(char) for char in example] for example in elon_data]\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "dataset = MyDataset(int_data)\n",
    "\n",
    "# Create the dataloader using the dataset\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_to_char(softmax):\n",
    "    return np.argmax(softmax)\n",
    "\n",
    "# output dist vs actual disturbution\n",
    "def loss_function(predictions, targets):\n",
    "    return nn.CrossEntropyLoss()(predictions, targets)\n",
    "\n",
    "def target_seq_to_distribution(value):\n",
    "    distribution = torch.zeros((dict_size))\n",
    "    value = int(value) -1\n",
    "    distribution[value] = 1.0\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 397])\n",
      "torch.Size([100, 397])\n",
      "tensor(5.9853, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "decoder = Decoder(layers=layers, epochs=epochs, batch_size=batch_size, embedding_dim=embedding_dim, dict_size=dict_size, feed_forward_dim=feed_forward_dim, num_heads=num_heads, attention_dim=attention_dim)\n",
    "\n",
    "# Create an optimizer for the Decoder model\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "# Associate the optimizer with the Decoder object\n",
    "decoder.optimizer = optimizer\n",
    "\n",
    "# random number generator \n",
    "torch.manual_seed(0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        # torch random between 0 and 426\n",
    "        stop_seq = torch.randint(0,(batch.shape[1]-1), (1,))[0]\n",
    "        next_token = stop_seq + 1\n",
    "\n",
    "        # get the input and target sequences\n",
    "        input_seq = batch[:, :stop_seq]\n",
    "        target_seq = batch[:, stop_seq:next_token]\n",
    "\n",
    "        # zero out the gradients\n",
    "        decoder.optimizer.zero_grad()\n",
    "\n",
    "        output = []\n",
    "\n",
    "        # shitty implementation, but it works for now (no batching implemented in actual transformer)\n",
    "        for i in range(batch_size):\n",
    "            x = decoder.forward(input_seq[i])\n",
    "            # get last output \n",
    "            output.append(x[-1])\n",
    "\n",
    "        # convert to tensor\n",
    "        output_tensor = torch.stack(output)\n",
    "        \n",
    "        target = []\n",
    "        for i in range(batch_size):\n",
    "            target.append(target_seq_to_distribution(target_seq[i]))\n",
    "        \n",
    "        target_seq = torch.stack(target)\n",
    "\n",
    "        print(output_tensor.shape)\n",
    "        print(target_seq.shape)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_function(output_tensor, target_seq)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
